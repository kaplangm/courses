{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import random\n",
    "\n",
    "random.seed(137)\n",
    "rest = random.random()\n",
    "\n",
    "def weight(word):\n",
    "    # overfitted\n",
    "    if word == 'lerxst@wam.umd.edu':\n",
    "        return 100.0\n",
    "    if word == 'car':\n",
    "        return random.random()\n",
    "    if word == 'dog':\n",
    "        return - random.random()\n",
    "    return random.random()\n",
    "\n",
    "def has(word, text):\n",
    "    return word in text \n",
    "\n",
    "def feature(index):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applied Machine Learning\n",
    "\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- We have some dataset\n",
    "- We identify the problem and define the loss function\n",
    "- Then we minimize the total loss (empirical risk, or objective) using available (training) data\n",
    "- We vary parameters to minimize the objective function\n",
    "- The minimizing parameters are then used to predict unknown values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A text classification problem\n",
    "\n",
    "Lets consider the **20 newsgroups** dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.autos\n",
      "----\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "text, label = data['data'][0], data['target_names'][data['target'][0]]\n",
    "print(label)\n",
    "print('----')\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A linear model for classification\n",
    "\n",
    "Let us consider a function that tells if the `text` comes from `rec.autos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5874950476268953"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight('car')*has('car', text) + weight('and')*has('and', text) + rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively say `car` is `0` and `dog` is `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6520663252475314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight(0)*feature(0) + weight(1)*feature(1) + rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How do we find those `weight` ($w$) for all the words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- Last time we used `opt.fmin` and it magically found the solution\n",
    "- The method is simple though\n",
    "- Start with random weights $w_0$\n",
    "- Iterate: $w_{i+1} = w_{i} - \\alpha \\times \\nabla \\mathsf{objective}(w_i)$\n",
    "- Approximate gradient: $\\nabla f(x) \\sim \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}$\n",
    "- All we need to know is the gradient of objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient of loss\n",
    "\n",
    "- Last time we considered a regression problem and used $(y-p)^2$\n",
    "- The gradient w.r.t $p$ is obvious: $- 2 (y - p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient check\n",
    "\n",
    "How can we ensure the gradient is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.39999999999999997, -0.400000000000001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, p):\n",
    "    return (y-p)**2\n",
    "\n",
    "def gradient(y, p):\n",
    "    return -2*(y-p)\n",
    "\n",
    "p = 0.1\n",
    "y = 0.3\n",
    "eps = 0.001\n",
    "gradient(y, p), (loss(y, p+eps) - loss(y, p-eps)) / (2*eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 0.14158068208195818\n",
      "0 0.23663227283278326\n",
      "1 0.2727518773180968\n",
      "2 0.28750673575034735\n",
      "3 0.29393358321197494\n",
      "4 0.29689826384534573\n",
      "5 0.29833830243112136\n",
      "6 0.29907120267565374\n",
      "7 0.29946037096291556\n",
      "8 0.29967517152002676\n",
      "9 0.2997980052946857\n"
     ]
    }
   ],
   "source": [
    "current_p = random.random()\n",
    "print('Initial', current_p)\n",
    "alpha = 0.3 # learning rate\n",
    "for i in range(10):\n",
    "    current_p = current_p - alpha*(0.95)**i*gradient(y, current_p)\n",
    "    print(i, current_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#current_p = random.random()\n",
    "current_weigths = [1, 1, 1]\n",
    "alpha = 0.1\n",
    "\n",
    "xs = list(range(20))\n",
    "ys = []\n",
    "for _ in xs:\n",
    "    current_weights = current_p - alpha*gradient(y, predict(current_weights))\n",
    "    ys.append(current_p)\n",
    "    \n",
    "plt.plot(xs, ys); plt.hlines(y, xs[0], xs[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification loss\n",
    "\n",
    "- We will use something called **logistic loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 144.26950408889635)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, p):\n",
    "    return np.log2(1.0 + np.exp(-y*p))\n",
    "    \n",
    "loss(-1, -100.0), loss(-1, +100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.67918581, -9.78951397]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.linear_model.SGDClassifier(loss='log')\n",
    "example_1 = [1,0]; label_1 = [1]\n",
    "example_2 = [0,1]; label_2 = [0]\n",
    "model.fit([example_1, example_2], np.ravel([label_1, label_2]))\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "\n",
    "- We can always come up with a model that fits data perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight('lerxst@wam.umd.edu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For some reason that's not what we want. Why?\n",
    "- First, we need to measure if such a thing happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Splitting the data\n",
    "\n",
    "- Obviously we should not test what we fit against\n",
    "- We should fit (train) the model on some part of data\n",
    "- Next, we check the model against the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave-on-out\n",
    "\n",
    "- Generate as many samples as there are examples\n",
    "- Gives you a good estimate if you don't have a lot of data\n",
    "- Gets impractical on huge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4] [0]\n",
      "[0 2 3 4] [1]\n",
      "[0 1 3 4] [2]\n",
      "[0 1 2 4] [3]\n",
      "[0 1 2 3] [4]\n"
     ]
    }
   ],
   "source": [
    "loo = sklearn.model_selection.LeaveOneOut()\n",
    "for train, test in loo.split([1,2,3,4,5]):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation\n",
    "\n",
    "- Split the dataset into a few (say 5) non-overlapping parts\n",
    "- Four parts go to training data and one part goes to test data\n",
    "- Do the above 5 times to train the model and test it\n",
    "- 5 runs (precision): 0.9, 0.89, 0.91, 0.92, 0.88 -> mean 0.9 \n",
    "- Makes a decent way to *detect* overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation in sklearn\n",
    "\n",
    "Let's consider indices of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4] [3 5]\n",
      "[1 3 4 5] [0 2]\n",
      "[0 2 3 5] [1 4]\n"
     ]
    }
   ],
   "source": [
    "xval = sklearn.model_selection.KFold(n_splits=3, shuffle=True, random_state=7)\n",
    "for train, test in xval.split([1,2,3,4,5,6]):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This thing is an ill-posed problem\n",
    "\n",
    "- A mathematical problem is ill-posed when the solution is not unique\n",
    "- That's exactly the case of regression/classification/...\n",
    "- We need to make the problem well-posed: *regularization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structural risk minimization\n",
    "\n",
    "- Structural risk is empirical risk plus regularizer\n",
    "- Instead of minimizing empirical risk we find some tradeoff\n",
    "- Regularizer is a function of model we get\n",
    "- $\\mathsf{objective} = \\mathsf{loss} + \\mathsf{regularizer}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularizer\n",
    "\n",
    "- A functions that reflects the complexity of a model\n",
    "- What is the complexity of a set of 'if ... then'?\n",
    "- Not obvious for linear model but easy to invent something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_1$ regularizer\n",
    "\n",
    "- $\\ell_1: \\| w \\|_1 = |w(car)| + |w(dog)| + ...$\n",
    "- Derivative is const\n",
    "- Forces weight to be zero if it doesn't hurt performance much \n",
    "- Use if you believe some features are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = sklearn.linear_model.SGDClassifier(loss='log', penalty='l1');\n",
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='l1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_2$ regularizer\n",
    "\n",
    "- $\\ell_2: \\| w \\|_2 = w(car)^2 + w(dog)^2 + ...$\n",
    "- Derivative is linear\n",
    "- Forces weights to get *similar* magnitude if it doesn't hurt performance much\n",
    "- Use if you believe all features are more or less important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = sklearn.linear_model.SGDClassifier(loss='log', penalty='l2');\n",
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='l2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic net\n",
    "\n",
    "- Just a weighted sum of $\\ell_1$ and $\\ell_2$ regularizers\n",
    "- An attempt to get useful properties of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='elasticnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of linearity\n",
    "\n",
    "- In low-dimensional spaces linear models are not very 'powerful' (can we define that?)\n",
    "- The higher dimensionality, the more powerful linear model becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse features\n",
    "\n",
    "- We say features are sparse when most of the values are zero\n",
    "- Examples: visited hosts, movies that user liked, ...\n",
    "- Sparse features are efficient in high-dimensional setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0, 0, ..., 1, ..., 0, 0, 1, 0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One hot encoding, hashing trick\n",
    "\n",
    "- One way to encode categorical things like visited websites\n",
    "- We enumerate all the websites\n",
    "- We put 1 to position of every host, 0 otherwise\n",
    "- Hashing trick: instead of enumerating them just hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7067579454498743199\n",
      "56417\n"
     ]
    }
   ],
   "source": [
    "print(hash('hse.ru'))\n",
    "print(hash('hse.ru') % 2**16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hashing vectorizer in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.70710678 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.70710678 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=10, binary=True)\n",
    "features = vectorizer.fit_transform(['hello there', 'hey there'])\n",
    "print(features.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When do we use linear models?\n",
    "\n",
    "- It is definitely the first thing to try if you have some text data\n",
    "- In general a good choice for any sparse data\n",
    "- This approach is pretty much the fastest one\n",
    "- Even if some method outperforms, you still get a good baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-assesment questions\n",
    "\n",
    "1. You noticed that your linear model learned a weight of **95.3** for the word `the`. *Is there a problem? Y/N*\n",
    "2. The train loss is **0.43** and the test loss is **0.39**. *Is it an example of ..? a) overfitting b) underfitting c) I don't know*\n",
    "3. You've got basically infinite amounts of data. *Do you have to use regularization? Y/N*\n",
    "4. You believe your dataset is pretty noisy and some features are broken. *You use a) L1 b) L2 c) no regularization* \n",
    "5. Why do we hash words? *a) it's simpler b) it's faster c) it's more reliable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Homework 1\n",
    "\n",
    "- No score, just has to be done\n",
    "- Load dataset, create linear model, train, and explain the results\n",
    "- The template is provided: `HSE-AML-HW1.ipynb`\n",
    "- Hint: check the code examples for `KFold`, `HashingVectorizer`, `LogisticRegression`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
